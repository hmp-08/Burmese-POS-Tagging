{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Z_7gnGipsAj",
    "outputId": "60f6aa2a-3e5a-4d0d-8bdd-4c269aece721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aIrzV8JHqxtZ",
    "outputId": "32082c96-3215-43ce-c7d5-6f4e74bafd3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting distlib<1,>=0.3.6 (from virtualenv)\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.12.0)\n",
      "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.3.0)\n",
      "Installing collected packages: distlib, virtualenv\n",
      "Successfully installed distlib-0.3.6 virtualenv-20.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Od9HbV5Uq1_c",
    "outputId": "71b48be5-ea07-437e-9c18-d33d19bb6fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created virtual environment CPython3.10.11.final.0-64 in 790ms\n",
      "  creator CPython3Posix(dest=/content/venv, clear=False, no_vcs_ignore=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
      "    added seed packages: pip==23.1.2, setuptools==67.7.2, wheel==0.40.0\n",
      "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
     ]
    }
   ],
   "source": [
    "!virtualenv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dI5sAYP6q7Zo"
   },
   "outputs": [],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0Sn0eLFlq-fo",
    "outputId": "867f6de9-1823-43fb-e462-0190320d557a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting OpenNMT-tf[tensorflow]\n",
      "  Downloading OpenNMT_tf-2.31.0-py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.0/162.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ctranslate2<4,>=3.0 (from OpenNMT-tf[tensorflow])\n",
      "  Downloading ctranslate2-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.0/32.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (23.1)\n",
      "Collecting pyonmttok<2,>=1.29.0 (from OpenNMT-tf[tensorflow])\n",
      "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (6.0)\n",
      "Collecting rouge<2,>=1.0 (from OpenNMT-tf[tensorflow])\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Collecting sacrebleu<3,>=1.5.0 (from OpenNMT-tf[tensorflow])\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-addons<0.20,>=0.16 (from OpenNMT-tf[tensorflow])\n",
      "  Downloading tensorflow_addons-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow<2.12.0,>=2.6.0 (from OpenNMT-tf[tensorflow])\n",
      "  Downloading tensorflow-2.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-text<2.12.0,>=2.6.0 (from OpenNMT-tf[tensorflow])\n",
      "  Downloading tensorflow_text-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.0->OpenNMT-tf[tensorflow]) (1.22.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge<2,>=1.0->OpenNMT-tf[tensorflow]) (1.16.0)\n",
      "Collecting portalocker (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow])\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow]) (2022.10.31)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow]) (0.8.10)\n",
      "Collecting colorama (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow])\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow]) (4.9.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.54.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.8.0)\n",
      "Collecting keras<2.12,>=2.11.0 (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow])\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (16.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.3.0)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow])\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (67.7.2)\n",
      "Collecting tensorboard<2.12,>=2.11 (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow])\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow])\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.32.0)\n",
      "Collecting typeguard>=2.7 (from tensorflow-addons<0.20,>=0.16->OpenNMT-tf[tensorflow])\n",
      "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.13.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.17.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow])\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow])\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.2.2)\n",
      "Installing collected packages: typeguard, tensorflow-estimator, tensorboard-data-server, rouge, pyonmttok, protobuf, portalocker, keras, ctranslate2, colorama, tensorflow-addons, sacrebleu, OpenNMT-tf, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.12.0\n",
      "    Uninstalling tensorflow-estimator-2.12.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.0\n",
      "    Uninstalling tensorboard-data-server-0.7.0:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.2\n",
      "    Uninstalling tensorboard-2.12.2:\n",
      "      Successfully uninstalled tensorboard-2.12.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed OpenNMT-tf-2.31.0 colorama-0.4.6 ctranslate2-3.13.0 google-auth-oauthlib-0.4.6 keras-2.11.0 portalocker-2.7.0 protobuf-3.19.6 pyonmttok-1.37.1 rouge-1.0.1 sacrebleu-2.3.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.1 tensorflow-addons-0.19.0 tensorflow-estimator-2.11.0 tensorflow-text-2.11.0 typeguard-4.0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install OpenNMT-tf[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzYYemQfL8Px"
   },
   "outputs": [],
   "source": [
    "!unzip -q dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jH4tWKZZMcMU",
    "outputId": "45d64b18-0cd3-4223-f287-0a654d8ae5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/dataset\n"
     ]
    }
   ],
   "source": [
    "%cd dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vHaZhsMsmpz",
    "outputId": "e3eba3b2-86e6-493e-f0ed-c6a410116d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.yml      sp-tgt.vocab         src-val.txt    train-alignment.txt\n",
      "sp-src.model  src-test.txt         tgt-test.txt\n",
      "sp-src.vocab  src-tgt-aligned.txt  tgt-train.txt\n",
      "sp-tgt.model  src-train.txt        tgt-val.txt\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIQzChW8st6S",
    "outputId": "773cfc39-4775-4702-9270-57fdf3ef3828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ရောဂါ ကြောင့် တစ် ရက် နား သည် ။\n",
      "သူ က ကတိမတည် လို့ ခေါင်း တောင် မ ဖော် ရဲ ဘူး ။\n",
      "ခင်ဗျား ဘာ အစီအစဉ် ရှိ သလဲ ။\n",
      "စင်္ကာပူ ကြည်းတပ် သည် စင်္ကာပူ တပ်မတော် ၏ တပ်ဖွဲ့ ကြီး သုံး ခု အနက် အဓိက တပ်ဖွဲ့ ဖြစ် သည် ။\n",
      "ကောင်း ပါ ပြီ ၊ ဆရာ ။\n",
      "သူ များ အိမ်တွင်းရေး ကို နှိုက်နှိုက်ချွတ်ချွတ် မ မေး ပါ နဲ့ ။\n",
      "ကျွန်တော့် ရဲ့ ဝါသနာ တစ် ရပ် က ကျွန်တော့် ရဲ့ စာရေး မိတ်ဆွေ တွေ ဆီ ကို စာ များ ရေး ခြင်း ဖြစ် ပါ တယ် ။\n",
      "မိန် သည် ရာသီ ဖြစ် သည် ။\n",
      "ရထား ထဲ မှာ အရမ်း ပူ ပြီး ပြည့်ကျပ် နေ တယ် ။\n",
      "စဲန် မြစ် သည် ပါရီ ၏ မြို့ ဟောင်း ဧရိယာ ကို ပိုင်းဖြတ် ပြီး ၂ ပိုင်း ဘယ် ကမ်း နှင့် ညာ ကမ်း ခွဲ ထား သည် ။\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 src-train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmLwqv9Esylh",
    "outputId": "c9463a65-fb69-48c9-f97c-be469d746614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n ppm tn n v ppm punc\n",
      "pron ppm v part n ppm part v part part punc\n",
      "pron adj n v part punc\n",
      "n n ppm n n ppm n part tn part ppm n n v ppm punc\n",
      "adj part ppm punc n punc\n",
      "pron part n ppm adv part v part part punc\n",
      "pron ppm n tn part ppm pron ppm v n part ppm ppm n part v part v part ppm punc\n",
      "n ppm n v ppm punc\n",
      "n ppm ppm adv v conj v part ppm punc\n",
      "n n ppm n ppm n adj n ppm v conj num part n n conj n n v part ppm punc\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 tgt-train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olgFwF6Ps5Kg",
    "outputId": "f2d61607-c18a-4073-e9c8-c00c3c50bd7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38876 src-train.txt\n",
      "38876 tgt-train.txt\n",
      "2160 src-val.txt\n",
      "2160 tgt-val.txt\n",
      "2160 src-test.txt\n",
      "2160 tgt-test.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l src-train.txt\n",
    "!wc -l tgt-train.txt\n",
    "!wc -l src-val.txt\n",
    "!wc -l tgt-val.txt\n",
    "!wc -l src-test.txt\n",
    "!wc -l tgt-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZ4spHdRtHYX",
    "outputId": "feca2cc2-4086-46f5-cb14-f653f5898d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 04:02:23.912985: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 04:02:24.941175: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-05-26 04:02:24.941323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-05-26 04:02:24.941343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-26 04:02:29.712728: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 04:02:31.337880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-05-26 04:02:31.338053: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-05-26 04:02:31.338087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "!onmt-build-vocab --from_vocab sp-src.vocab --from_format sentencepiece --save_vocab sp_src_vocab.txt\n",
    "!onmt-build-vocab --from_vocab sp-tgt.vocab --from_format sentencepiece --save_vocab sp_tgt_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQy05aFFtd3V",
    "outputId": "0c17a3ac-33e3-4f9f-b44f-d84ce5a126a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<blank>\n",
      "<s>\n",
      "</s>\n",
      "▁။\n",
      "▁\n",
      "▁ကို\n",
      "▁သည်\n",
      "▁ပါ\n",
      "▁တယ်\n",
      "▁က\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 sp_src_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7lU_yqktjYG",
    "outputId": "cb868ce1-0a50-4723-fc81-7d8a339e1580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<blank>\n",
      "<s>\n",
      "</s>\n",
      "▁part\n",
      "▁n\n",
      "▁ppm\n",
      "▁v\n",
      "▁punc\n",
      "▁p\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 sp_tgt_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhiOZ_gWtok1",
    "outputId": "c985e333-96bd-42d5-f295-fce260e36914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sp_src_vocab.txt\n",
      "47 sp_tgt_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l sp_src_vocab.txt\n",
    "!wc -l sp_tgt_vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSntzvPLtz_c"
   },
   "source": [
    "Then, the data files should be declared in a YAML configuration file, let’s name it data.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSyqfY1KuUBV",
    "outputId": "b363132a-51b6-41fa-ef7e-55702771e69f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 04:03:38.887626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 04:03:39.909188: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-05-26 04:03:39.909287: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-05-26 04:03:39.909305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-26 04:03:43.056000: I onmt-main:8] Creating model directory run/\n",
      "2023-05-26 04:03:43.285000: I main.py:315] Using OpenNMT-tf version 2.31.0\n",
      "2023-05-26 04:03:43.286000: I main.py:315] Using model:\n",
      "(model): TransformerBase(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2023-05-26 04:03:44.578458: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-05-26 04:03:44.585000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: src-val.txt\n",
      "  eval_labels_file: tgt-val.txt\n",
      "  source_tokenization:\n",
      "    params:\n",
      "      model: sp-src.model\n",
      "    type: SentencePieceTokenizer\n",
      "  source_vocabulary: sp_src_vocab.txt\n",
      "  target_tokenization:\n",
      "    params:\n",
      "      model: sp-tgt.model\n",
      "    type: SentencePieceTokenizer\n",
      "  target_vocabulary: sp_tgt_vocab.txt\n",
      "  train_features_file: src-train.txt\n",
      "  train_labels_file: tgt-train.txt\n",
      "early_stopping:\n",
      "  metric: bleu\n",
      "  min_improvement: 0.01\n",
      "  steps: 100\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  export_format: ctranslate2\n",
      "  export_on_best: bleu\n",
      "  length_bucket_width: 5\n",
      "  max_exports_to_keep: 5\n",
      "  scorers: bleu\n",
      "  steps: 100\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: run/\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 4\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 8000\n",
      "  decay_type: NoamDecay\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  num_hypotheses: 1\n",
      "  optimizer: LazyAdam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.9\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 32\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 4\n",
      "  length_bucket_width: 2\n",
      "  max_step: 10000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: -1\n",
      "  save_checkpoints_steps: 100\n",
      "  save_summary_steps: 100\n",
      "  train_steps: 500\n",
      "\n",
      "2023-05-26 04:03:44.916000: I inputter.py:316] Initialized source input layer:\n",
      "2023-05-26 04:03:44.916000: I inputter.py:316]  - vocabulary size: 10001\n",
      "2023-05-26 04:03:44.917000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2023-05-26 04:03:44.925000: I inputter.py:316] Initialized target input layer:\n",
      "2023-05-26 04:03:44.925000: I inputter.py:316]  - vocabulary size: 48\n",
      "2023-05-26 04:03:44.925000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2023-05-26 04:03:44.933000: W runner.py:268] No checkpoint to restore in run/\n",
      "2023-05-26 04:03:44.936000: W deprecation.py:350] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2023-05-26 04:03:45.041000: W deprecation.py:350] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2023-05-26 04:03:47.414000: I main.py:325] Accumulate gradients of 782 iterations to reach effective batch size of 25000\n",
      "2023-05-26 04:03:47.459000: I dataset_ops.py:2542] Training on 38876 examples\n",
      "2023-05-26 04:04:52.610000: I runner.py:309] Number of model parameters: 49310256\n",
      "2023-05-26 04:04:52.620000: I runner.py:309] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2023-05-26 04:04:57.121000: I training.py:176] Saved checkpoint run/ckpt-1\n",
      "2023-05-26 04:47:10.955000: I runner.py:309] Step = 100 ; steps/s = 0.04, tokens/s = 1580 (802 source, 778 target) ; Learning rate = 0.000012 ; Loss = 2.138792\n",
      "2023-05-26 04:47:17.621000: I training.py:176] Saved checkpoint run/ckpt-100\n",
      "2023-05-26 04:47:17.621000: I training.py:192] Running evaluation for step 100\n",
      "2023-05-26 04:48:31.291000: I training.py:192] Evaluation predictions saved to run/eval/predictions.txt.100\n",
      "2023-05-26 04:48:31.500000: I training.py:192] Evaluation result for step 100: loss = 1.592223 ; perplexity = 4.914662 ; bleu = 21.684840\n",
      "2023-05-26 04:48:31.506000: I training.py:192] Exporting model to run/export/100 (best bleu so far: 21.684840)\n",
      "2023-05-26 05:32:42.903000: I runner.py:309] Step = 200 ; steps/s = 0.04, tokens/s = 1527 (775 source, 752 target) ; Learning rate = 0.000025 ; Loss = 1.919640\n",
      "2023-05-26 05:32:49.433000: I training.py:176] Saved checkpoint run/ckpt-200\n",
      "2023-05-26 05:32:49.434000: I training.py:192] Running evaluation for step 200\n",
      "2023-05-26 05:34:11.432000: I training.py:192] Evaluation predictions saved to run/eval/predictions.txt.200\n",
      "2023-05-26 05:34:11.627000: I training.py:192] Evaluation result for step 200: loss = 1.367651 ; perplexity = 3.926116 ; bleu = 26.135528\n",
      "2023-05-26 05:34:11.632000: I training.py:192] Exporting model to run/export/200 (best bleu so far: 26.135528)\n",
      "2023-05-26 06:18:27.065000: I runner.py:309] Step = 300 ; steps/s = 0.04, tokens/s = 1524 (774 source, 750 target) ; Learning rate = 0.000037 ; Loss = 1.794523\n",
      "2023-05-26 06:18:33.797000: I training.py:176] Saved checkpoint run/ckpt-300\n",
      "2023-05-26 06:18:33.797000: I training.py:192] Running evaluation for step 300\n",
      "2023-05-26 06:19:50.917000: I training.py:192] Evaluation predictions saved to run/eval/predictions.txt.300\n",
      "2023-05-26 06:19:51.127000: I training.py:192] Evaluation result for step 300: loss = 1.235644 ; perplexity = 3.440595 ; bleu = 32.565722\n",
      "2023-05-26 06:19:51.132000: I training.py:192] Exporting model to run/export/300 (best bleu so far: 32.565722)\n",
      "2023-05-26 07:03:42.509000: I runner.py:309] Step = 400 ; steps/s = 0.04, tokens/s = 1538 (781 source, 757 target) ; Learning rate = 0.000050 ; Loss = 1.688654\n",
      "2023-05-26 07:03:45.981000: I training.py:176] Saved checkpoint run/ckpt-400\n",
      "2023-05-26 07:03:45.981000: I training.py:192] Running evaluation for step 400\n",
      "2023-05-26 07:05:07.971000: I training.py:192] Evaluation predictions saved to run/eval/predictions.txt.400\n",
      "2023-05-26 07:05:08.192000: I training.py:192] Evaluation result for step 400: loss = 1.124252 ; perplexity = 3.077914 ; bleu = 38.267210\n",
      "2023-05-26 07:05:08.200000: I training.py:192] Exporting model to run/export/400 (best bleu so far: 38.267210)\n",
      "2023-05-26 07:48:42.554000: I runner.py:309] Step = 500 ; steps/s = 0.04, tokens/s = 1549 (786 source, 763 target) ; Learning rate = 0.000062 ; Loss = 1.592503\n",
      "2023-05-26 07:48:49.614000: I training.py:176] Saved checkpoint run/ckpt-500\n",
      "2023-05-26 07:48:49.615000: I training.py:192] Running evaluation for step 500\n",
      "2023-05-26 07:49:42.638000: I training.py:192] Evaluation predictions saved to run/eval/predictions.txt.500\n",
      "2023-05-26 07:49:42.977000: I training.py:192] Evaluation result for step 500: loss = 1.033440 ; perplexity = 2.810717 ; bleu = 43.215651\n",
      "2023-05-26 07:49:42.983000: I training.py:192] Exporting model to run/export/500 (best bleu so far: 43.215651)\n",
      "2023-05-26 08:33:16.784000: I runner.py:309] Step = 600 ; steps/s = 0.04, tokens/s = 1549 (786 source, 763 target) ; Learning rate = 0.000074 ; Loss = 1.495064\n",
      "2023-05-26 08:33:27.029000: I training.py:176] Saved checkpoint run/ckpt-600\n",
      "2023-05-26 08:33:27.029000: I training.py:192] Running evaluation for step 600\n",
      "2023-05-26 08:34:08.109000: I training.py:192] Evaluation predictions saved to run/eval/predictions.txt.600\n",
      "2023-05-26 08:34:08.360000: I training.py:192] Evaluation result for step 600: loss = 0.918436 ; perplexity = 2.505369 ; bleu = 46.860157\n",
      "2023-05-26 08:34:08.365000: I training.py:192] Exporting model to run/export/600 (best bleu so far: 46.860157)\n",
      "2023-05-26 09:18:16.943000: I runner.py:309] Step = 700 ; steps/s = 0.04, tokens/s = 1529 (776 source, 753 target) ; Learning rate = 0.000087 ; Loss = 1.373851\n",
      "2023-05-26 09:18:20.691000: I training.py:176] Saved checkpoint run/ckpt-700\n",
      "2023-05-26 09:18:20.692000: I training.py:192] Running evaluation for step 700\n",
      "2023-05-26 09:19:01.734000: I training.py:192] Evaluation predictions saved to run/eval/predictions.txt.700\n",
      "2023-05-26 09:19:01.994000: I training.py:192] Evaluation result for step 700: loss = 0.757458 ; perplexity = 2.132848 ; bleu = 57.585753\n",
      "2023-05-26 09:19:01.999000: I training.py:192] Exporting model to run/export/700 (best bleu so far: 57.585753)\n"
     ]
    }
   ],
   "source": [
    "!onmt-main --model_type Transformer --config data.yml --auto_config train --with_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b04vpIaIvX6E"
   },
   "outputs": [],
   "source": [
    "!onmt-main --config data.yml --auto_config infer --features_file src-test.txt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
